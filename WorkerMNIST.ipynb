{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_models import Classifier, init_weights, accuracy, validate\n",
    "from communicator import Server \n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import threading\n",
    "\n",
    "%matplotlib inline\n",
    "torch.manual_seed(0)  # for reproducibility\n",
    "dataset_size = 60000\n",
    "\n",
    "\n",
    "################################################\n",
    "#               META INFO                      #\n",
    "################################################\n",
    "WORKER_ID = 0  # id from 0 to 4\n",
    "perm_idxs = torch.randperm(dataset_size)\n",
    "worker_idxs = perm_idxs[WORKER_ID*10000:(WORKER_ID+1)*10000]\n",
    "\n",
    "################################################\n",
    "#               Get Datasets                   #\n",
    "################################################\n",
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', download=True, train=True, transform=transforms.ToTensor())\n",
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 128\n",
    "train_ds = Subset(dataset, worker_idxs)\n",
    "train_data = dataset.data[worker_idxs]\n",
    "train_target = dataset.targets[worker_idxs]\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_ds, batch_size)\n",
    "\n",
    "################################################\n",
    "#            Get Models and Initialize         #\n",
    "################################################\n",
    "model = Classifier(hidden_1=0)\n",
    "model.apply(init_weights)\n",
    "\n",
    "lr = 0.001\n",
    "lossfn = F.cross_entropy\n",
    "opt = torch.optim.SGD(model.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#    Networking Stuff (only run this once!)    #\n",
    "################################################\n",
    "PORT = 1232\n",
    "PORT2 = 1233\n",
    "HOSTADDR = \"container1\"\n",
    "serv = Server(PORT)\n",
    "loc_model = (HOSTADDR, PORT)\n",
    "loc_server = (HOSTADDR, PORT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 110] Connection timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d3d31f42f1cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mgrad_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#     print(grad_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mserv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_server\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/communicator.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, location, msgtosend)\u001b[0m\n\u001b[1;32m     65\u001b[0m \t\t\"\"\"\n\u001b[1;32m     66\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_INET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgtosend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{len(msg):<{HEADERSIZE}}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "dataloader_iterator = iter(train_loader)\n",
    "\n",
    "while True:\n",
    "#     enter = input('Hit ENTER to send update:')\n",
    "#     while enter != '':\n",
    "#         enter = input('Hit ENTER to send update:')\n",
    "    # send request to server for model w\n",
    "    msg = \"Req\"\n",
    "    serv.send(loc_model, msg)\n",
    "#     print(f\"Sent {str(msg)}\")\n",
    "\n",
    "    # wait for model \n",
    "    server_state_dict = serv.recv()\n",
    "    # get model and load\n",
    "    model.load_state_dict(server_state_dict)\n",
    "    \n",
    "    # now compute gradient and send state_dict\n",
    "    # first get a sample\n",
    "    try:\n",
    "        data, target = next(dataloader_iterator)\n",
    "    except StopIteration:\n",
    "        dataloader_iterator = iter(train_loader)\n",
    "        data, target = next(dataloader_iterator)\n",
    "    # zero out grad\n",
    "    opt.zero_grad()\n",
    "    # get prediction from current (latest) model\n",
    "    pred = model(data)\n",
    "    loss = lossfn(pred, target)\n",
    "    loss.backward()\n",
    "    # now the gradients are stored inside the state_dict. We send grad_dict\n",
    "    grad_dict = {k:v.grad for k, v in zip(model.state_dict(), model.parameters())}\n",
    "#     print(grad_dict)\n",
    "    serv.send(loc_server, grad_dict)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
